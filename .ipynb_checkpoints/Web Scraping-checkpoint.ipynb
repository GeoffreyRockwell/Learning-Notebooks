{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "This notebook shows how to scrape web pages to build a text corpus. It draws on Ryan Mitchell's [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do) (Sebastopol, CA, O'Reilly, 2015.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "Here are the libraries we want to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding list of Links\n",
    "\n",
    "Now we need to find a list of links to scrape. In this case we are using a CSV as then we can use it to keep track. Then we load the second column into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleTable.csv         Second Notebook.ipynb    Web Scraping.ipynb\r\n",
      "Hume Enquiry.txt         Test.txt\r\n",
      "My First Notebook.ipynb  Third Notebook.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://philosophi.ca', 'http://theoreti.ca']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "with open('ExampleTable.csv', 'r') as file: # This makes sure that file is closed after reading\n",
    "    data = csv.reader(file)\n",
    "    for row in data:\n",
    "        links.append(row[1]) # This puts all the data into a list\n",
    "file.closed\n",
    "urlLinks = links[1:] # This gets all items but the first (which is a label)\n",
    "urlLinks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing one link\n",
    "\n",
    "Here is a function to get one link and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getURL(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as error:\n",
    "        return \"HTTPError: \" + str(error)\n",
    "    except URLError as error:\n",
    "        return \"URLError: \" + str(error)\n",
    "    else:\n",
    "        bsObject = BeautifulSoup(html.read(), \"lxml\")\n",
    "        return bsObject.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing with One Link\n",
    "Here is code to get one link. We won't use this for the large scale scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Theoreti.ca  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Theoreti.ca\n",
      "Research notes taken on subjects around multimedia, electronic texts, and computer games.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "My Digital Humanities\n",
      "October 13th, 2016 \n",
      "\n",
      "\n",
      "The folks at #dariah Teach have put together a first of a series of videos on My Digital Humanities. Despite appearing in it, the video seems very nicely produced and there is a nice mix of people. Stéfan Sinclair and I were interviewed together, something that isn’t clear in the first part, but will presumably become clear later.\n",
      "\n",
      "Posted in Conference, Humanities Computing, Streaming Media |    Comments Off\n",
      "\n",
      "\n",
      "FBI Game: What is Violent Extremism?\n",
      "October 12th, 2016 \n",
      "\n",
      "\n",
      "From Slashdot a story about an FBI game/interactive that is online and which aims at Countering Violent Extremism | What is Violent Extremism?. The subtitle is “Don’t Be A Puppet” and the game is part of a collection of interactive materials that try to teach about extremism in general and encourage some critical distance from the extremism. The game has you as a sheep avoiding pitfalls.\n",
      " Read the rest of this entry »\n",
      "\n",
      "Posted in Computer Games, Computers and Education, Interactives, Surveillance |    Comments Off\n",
      "\n",
      "\n",
      "Geofeedia ‘allowed police to track protesters’\n",
      "October 12th, 2016 \n",
      "\n",
      "\n",
      "From the BBC a story about US start-up Geofeedia ‘allowed police to track protesters’. Geofeedia is apparently using social media data from Twitter, Facebook and Instagram to monitor activists and protesters for law enforcement. Access to these social media was changed once the ACLU reported on the surveillance product. The ACLU discovered the agreements with Geofeedia when they requested public records of California law enforcement agencies. Geofeedia was boasting to law enforcement about their access. The ACLU has released some of the documents of interest including a PDF of a Geofeedia Product Update email discussing “sentiment” analytics (May 18, 2016).\n",
      "Frome the Geofeedia web site I was surprised to see that they are offering solutions for education too.\n",
      "\n",
      "Posted in Computers and Education, Media and News, Social Networking, Surveillance, Text Analysis, Visualization |    Comments Off\n",
      "\n",
      "\n",
      "Access 2016\n",
      "October 8th, 2016 \n",
      "\n",
      "Yesterday I gave a talk at Access 2016. This conference brings together archivists and librarians interested in library technology. I was honoured to give the Dave Binkley Memorial Lecture at the end of the conference. My conference notes are here. My talk was about the ethics of digitization, or more generally datafication.\n",
      "\n",
      "Posted in Conference, Digital Archives, Ethics of Data Science, Markup and Text Representation |    Comments Off\n",
      "\n",
      "\n",
      "Mastaba Snoopy\n",
      "October 5th, 2016 \n",
      "\n",
      "From my students I heard about the game Mastaba Snoopy created in Twee and TiddlyWiki and being taught in another Humanities Computing course (our students are vectors of influence.) Here is a review where you can download the single HTML page that is the bizarre text adventure, Mastaba Snoopy is a Cronenbergian nightmare vision of childhood. The story takes place14,000 years in the future when a mutable alien has destroyed us and then reinvented itself following a collection of Peanuts comics. Play it.\n",
      "\n",
      "Posted in Computer Games, Hypertext Fiction, Literature |    Comments Off\n",
      "\n",
      "\n",
      "Common Errors in English Usage\n",
      "October 5th, 2016 \n",
      "\n",
      "An article about authorship attribution led me to this nice site on Common Errors in English Usage. The site is for a book with that title, but the author Paul Brians has organized all the errors into a hypertext here. For example, here is the entry on why you shouldn’t use enjoy to.\n",
      "What does this have to do with authorship attribution? In a paper on Authorship Identification on the Large Scale the authors try using common errors as feature to discriminate potential authors.\n",
      "\n",
      "Posted in Humanities Computing, Literature, Programming, Stylistics, Text Analysis |    Comments Off\n",
      "\n",
      "\n",
      "Instant History conference\n",
      "September 28th, 2016 \n",
      "\n",
      "This weekend I gave a talk at a lovely one day conference on Instant History, The Postwar Digital Humanities and Their Legacies. My conference notes are here. The conference was organized by Paul Eggert, among others. Steve Jones, Ted Underwood and Laura Mandell also talked.\n",
      "I gave the first talk on “Tremendous Labour: Busa’s Methods” – a paper coming from the work Stéfan Sinclair and I are doing. I talked about the reconstruction of Busa’s Index project. I claimed that Busa and Tasman made two crucial innovations. The first was figuring out how to represent data on punched cards so that it could be processed (the data structures). The second was figuring out how to use the punched card machines at hand to tokenize unstructured text. I walked through what we know about their actual methods and talked about our attempts to replicate them:\n",
      "\n",
      "Simple Punched Card Emulator\n",
      "Busa Operations\n",
      "\n",
      "I was lucky to have two great respondents (Kyle Roberts and Schlomo Argamon) who both pointed out important contextual issues to consider, as in:\n",
      "\n",
      "We need to pay attention to the Jesuit and spiritual dimensions of Busa’s work.\n",
      "We need to think about the dialectic of those critical of computing and those optimistic about it.\n",
      "\n",
      "\n",
      "Posted in Conference, History of Computing and Multimedia, Humanities Computing, Markup and Text Representation, Programming |    Comments Off\n",
      "\n",
      "\n",
      "CWRC/CSEC: The Canadian Writing Research Collaboratory\n",
      "September 22nd, 2016 \n",
      "\n",
      "\n",
      "The Canadian Writing Research Collaboratory (CWRC) today launched its Collaboratory. The Collaboratory is a distributed editing environment that allows projects to edit scholarly electronic texts (using CWRC Writer), manage editorial workflows, and publish collections. There are also links to other tools like CWRC Catalogue and Voyant (that I am involved in.) There is an impressive set of projects already featured in CWRC, but it is open to new projects and designed to help them.\n",
      "Susan Brown deserves a lot of credit for imagining this, writing the CFI (and other) proposals, leading the development and now managing the release. I hope it gets used as it is a fabulous layer of infrastructure designed by scholars for scholars.\n",
      "One important component in CWRC is CWRC-Writer, an in-browser XML editor that can be hooked into content management systems like the CWRC back-end. It allows for stand-off markup and connects to entity databases for tagging entities in standardized ways.\n",
      "\n",
      "Posted in Conference, Humanities Computing, Interface Design and Usability, Internet Culture and Technology, Literature, Markup and Text Representation, Text Analysis |    Comments Off\n",
      "\n",
      "\n",
      "Monopoly’s Inventor: The Progressive Who Didn’t Pass ‘Go’ – The New York Times\n",
      "September 14th, 2016 \n",
      "\n",
      "Elizabeth Magie\n",
      "I had read somewhere that Monopoly had originally been developed to teach the evils of monopolies, but hadn’t realized how interesting the story of the creation of Monopoly was. The New York Times tells the story in, Monopoly’s Inventor: The Progressive Who Didn’t Pass ‘Go’. This excerpted from a book titled, The Monopolists: Obsession, Fury, and the Scandal Behind the World’s Favorite Board Game. The article tells the story of Elizabeth Magie who developed a game called the Landlord’s Game which had two sets of rules to teach about the alternatives to monopoly capitalism. You play the game with a rule set where the monopolists get richer and then with a rule set where wealth is distributed more fairly. Alas, when Darrow adapted the game and sold it to Parker Brothers he left out the progressive side.\n",
      "It strikes me as an interesting example where a game designed for a serious purpose gets adapted to be more fun and in the process loses its progressive purpose. A change in the rules and you don’t have a game that teaches.\n",
      "\n",
      "Posted in Computer Games, History of Computing and Multimedia |    Comments Off\n",
      "\n",
      "\n",
      "New mobile app issues verbal safety warnings to Edmonton drivers\n",
      "September 14th, 2016 \n",
      "\n",
      "Screen Shot of SmartTravel App\n",
      "Edmonton has released a new mobile app called SmartTravel. The new mobile app issues verbal safety warnings to Edmonton drivers. You can get live warnings as you drive or before. Interesting idea.\n",
      "\n",
      "Posted in Locative, Media and News, Mobile Computing |    Comments Off\n",
      "\n",
      "\n",
      "« Older Entries\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search for:\n",
      "\n",
      "\n",
      "\n",
      "Archives \n",
      "Select Month\n",
      " October 2016 \n",
      " September 2016 \n",
      " August 2016 \n",
      " July 2016 \n",
      " June 2016 \n",
      " May 2016 \n",
      " April 2016 \n",
      " March 2016 \n",
      " February 2016 \n",
      " January 2016 \n",
      " December 2015 \n",
      " November 2015 \n",
      " October 2015 \n",
      " September 2015 \n",
      " August 2015 \n",
      " July 2015 \n",
      " June 2015 \n",
      " May 2015 \n",
      " April 2015 \n",
      " March 2015 \n",
      " February 2015 \n",
      " January 2015 \n",
      " December 2014 \n",
      " November 2014 \n",
      " October 2014 \n",
      " September 2014 \n",
      " August 2014 \n",
      " July 2014 \n",
      " June 2014 \n",
      " May 2014 \n",
      " April 2014 \n",
      " March 2014 \n",
      " February 2014 \n",
      " January 2014 \n",
      " December 2013 \n",
      " November 2013 \n",
      " October 2013 \n",
      " September 2013 \n",
      " August 2013 \n",
      " July 2013 \n",
      " June 2013 \n",
      " May 2013 \n",
      " April 2013 \n",
      " March 2013 \n",
      " February 2013 \n",
      " January 2013 \n",
      " December 2012 \n",
      " November 2012 \n",
      " July 2012 \n",
      " June 2012 \n",
      " May 2012 \n",
      " April 2012 \n",
      " March 2012 \n",
      " February 2012 \n",
      " January 2012 \n",
      " December 2011 \n",
      " November 2011 \n",
      " October 2011 \n",
      " September 2011 \n",
      " August 2011 \n",
      " July 2011 \n",
      " June 2011 \n",
      " May 2011 \n",
      " April 2011 \n",
      " March 2011 \n",
      " February 2011 \n",
      " January 2011 \n",
      " December 2010 \n",
      " November 2010 \n",
      " October 2010 \n",
      " September 2010 \n",
      " August 2010 \n",
      " July 2010 \n",
      " June 2010 \n",
      " May 2010 \n",
      " April 2010 \n",
      " March 2010 \n",
      " February 2010 \n",
      " January 2010 \n",
      " December 2009 \n",
      " November 2009 \n",
      " October 2009 \n",
      " September 2009 \n",
      " August 2009 \n",
      " July 2009 \n",
      " June 2009 \n",
      " May 2009 \n",
      " April 2009 \n",
      " March 2009 \n",
      " February 2009 \n",
      " January 2009 \n",
      " December 2008 \n",
      " November 2008 \n",
      " October 2008 \n",
      " September 2008 \n",
      " August 2008 \n",
      " July 2008 \n",
      " June 2008 \n",
      " May 2008 \n",
      " April 2008 \n",
      " March 2008 \n",
      " February 2008 \n",
      " January 2008 \n",
      " December 2007 \n",
      " November 2007 \n",
      " October 2007 \n",
      " September 2007 \n",
      " August 2007 \n",
      " July 2007 \n",
      " June 2007 \n",
      " May 2007 \n",
      " April 2007 \n",
      " March 2007 \n",
      " February 2007 \n",
      " January 2007 \n",
      " December 2006 \n",
      " November 2006 \n",
      " October 2006 \n",
      " September 2006 \n",
      " August 2006 \n",
      " July 2006 \n",
      " June 2006 \n",
      " May 2006 \n",
      " April 2006 \n",
      " March 2006 \n",
      " February 2006 \n",
      " January 2006 \n",
      " December 2005 \n",
      " November 2005 \n",
      " October 2005 \n",
      " September 2005 \n",
      " August 2005 \n",
      " July 2005 \n",
      " June 2005 \n",
      " May 2005 \n",
      " April 2005 \n",
      " March 2005 \n",
      " February 2005 \n",
      " January 2005 \n",
      " December 2004 \n",
      " November 2004 \n",
      " October 2004 \n",
      " September 2004 \n",
      " August 2004 \n",
      " July 2004 \n",
      " June 2004 \n",
      " May 2004 \n",
      " April 2004 \n",
      " March 2004 \n",
      " February 2004 \n",
      " January 2004 \n",
      " December 2003 \n",
      " November 2003 \n",
      " October 2003 \n",
      " September 2003 \n",
      " August 2003 \n",
      " July 2003 \n",
      " June 2003 \n",
      "\n",
      "Categories\n",
      "Select Category\n",
      "Big Data\n",
      "Blogroll\n",
      "Blogs and Blogging Culture\n",
      "Book Arts\n",
      "Computer Games\n",
      "Computers and Education\n",
      "Conference\n",
      "Crowdsourcing\n",
      "Digital and Interactive Art\n",
      "Digital Archives\n",
      "Education and Administration\n",
      "Ethics of Data Science\n",
      "Fabrication\n",
      "Foundations of Computing\n",
      "High Performance Computing\n",
      "History of Computing and Multimedia\n",
      "Humanities Computing\n",
      "Hypertext Fiction\n",
      "Interactives\n",
      "Interface Design and Usability\n",
      "Internet Culture and Technology\n",
      "Japan\n",
      "Literature\n",
      "Locative\n",
      "Markup and Text Representation\n",
      "Media and News\n",
      "Methods\n",
      "Mobile Computing\n",
      "mobile post\n",
      "Multimedia Theory\n",
      "Online Publishing\n",
      "People\n",
      "Philosophy of Computing\n",
      "Photographs\n",
      "Playful or Cool\n",
      "Programming\n",
      "Project Management\n",
      "Science Fiction\n",
      "Semantic Web Technologies\n",
      "Social Innovation\n",
      "Social Networking\n",
      "Spam\n",
      "Streaming Media\n",
      "Stylistics\n",
      "Supercomputing\n",
      "Surveillance\n",
      "Text Analysis\n",
      "Text Technology and TAPoR\n",
      "Uncategorized\n",
      "Visualization\n",
      "Words in the Wild\n",
      "\n",
      "\n",
      "/* <![CDATA[ */\n",
      "\tvar dropdown = document.getElementById(\"cat\");\n",
      "\tfunction onCatChange() {\n",
      "\t\tif ( dropdown.options[dropdown.selectedIndex].value > 0 ) {\n",
      "\t\t\tlocation.href = \"http://theoreti.ca/?cat=\"+dropdown.options[dropdown.selectedIndex].value;\n",
      "\t\t}\n",
      "\t}\n",
      "\tdropdown.onchange = onCatChange;\n",
      "/* ]]> */\n",
      "\n",
      "Blogroll\n",
      "\n",
      "Data Mining\n",
      "Digital Ethnography\n",
      "Digital Humanities International\n",
      "Grand Text Auto\n",
      "Infolet\n",
      "jill/text\n",
      "Matt Kirschenbaum\n",
      "Melissa Terras\n",
      "Michael Geist\n",
      "Shawn Day’s Randomosity\n",
      "Stéfan Sinclair online\n",
      "Stephen Ramsay's Blog\n",
      "\n",
      "\n",
      "My Sites\n",
      "\n",
      "Flickr Photo Stream\n",
      "Geoffrey Rockwell\n",
      "TAPoR Portal\n",
      "\n",
      "\n",
      "Meta \n",
      "Log in\n",
      "Entries RSS\n",
      "Comments RSS\n",
      "WordPress.org \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "\tvar broaDHcast = new broaDHcast.Widget();\r\n",
      "\tbroaDHcast.render(\"fc0031af-72d7-4a74-a377-7691be39354f\"); \t\t\t\t\r\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tTheoreti.ca is powered by WordPress with White as Milk designed by Azeem Azeez. Entries (RSS) and Comments (RSS).\n",
      "\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(getURL(\"http://theoreti.ca\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Texts\n",
    "\n",
    "Here is a function to clean up the text a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanLines(theXt):\n",
    "    lineTokens = [line for line in theXt.split('\\n') if line.strip() != '']\n",
    "    theNewText = \"\\n\".join(lineTokens)\n",
    "    return theNewText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Over a List of Links\n",
    "\n",
    "This is the main loop that goes through the list of links, cleaning the text, and appending it to a text (simple XML) string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scrapes date = '17/10/2016'>\n",
      "<site link = 'http://philosophi.ca'>\n",
      "philosophi.ca : Home Page \n",
      "<!--\n",
      "  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n",
      "  code.escaped { white-space: nowrap; }\n",
      "  .vspace { margin-top:1.33em; }\n",
      "  .indent { margin-left:40px; }\n",
      "  .outdent { margin-left:40px; text-\n"
     ]
    }
   ],
   "source": [
    "theWebTexts = \"<scrapes\" + \" date = \\'\" + time.strftime(\"%d/%m/%Y\") + \"\\'>\\n\"\n",
    "for linkUrl in urlLinks:\n",
    "    theWebTexts += \"<site \" + \"link = \\'\" + linkUrl + \"\\'>\\n\"\n",
    "    theWebTexts += cleanLines(getURL(linkUrl))\n",
    "    theWebTexts += \"</site>\\n\"\n",
    "    \n",
    "theWebTexts += \"</scrapes>\"\n",
    "    \n",
    "print(theWebTexts[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Results\n",
    "\n",
    "Finally we write out the results as an XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "nameOfResults = \"ScrapeResults.xml\"\n",
    "\n",
    "with open(nameOfResults, \"w\") as fileToWrite:\n",
    "    fileToWrite.write(theWebTexts)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17/10/2016'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
